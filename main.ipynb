{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPHY6f6eH/c7tlFAc3vktfR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":37,"metadata":{"id":"ljao1-EtGQgI","executionInfo":{"status":"ok","timestamp":1651427384046,"user_tz":-180,"elapsed":699,"user":{"displayName":"Ερμης Αρβανιτης","userId":"15428110991140850064"}}},"outputs":[],"source":["import numpy as np\n","import re\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from pandas import concat\n","from sklearn.preprocessing import Normalizer, StandardScaler\n","from sklearn.model_selection import cross_val_score\n","import tensorflow as tf\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras.models import load_model\n","from sklearn import preprocessing\n","from sklearn.preprocessing import normalize, scale\n","from sklearn.model_selection import KFold\n","from tensorflow.compat.v1.losses import Reduction\n","from google.colab import files\n","from tensorflow.keras.layers import Dropout"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dd28b_ItGbEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = open(\"/content/drive/MyDrive/dataset/dataset/data/x_train.csv\")\n","x_train = np.loadtxt(data, delimiter=\",\")\n","#print(x_train)\n","\n","data = open(\"/content/drive/MyDrive/dataset/dataset/data/x_test.csv\")\n","x_test = np.loadtxt(data, delimiter=\",\")\n","#print(x_test)\n","\n","data = open(\"/content/drive/MyDrive/dataset/dataset/data/train-label.dat\")\n","y_train = np.loadtxt(data, delimiter=\" \", dtype='int')\n","#print(y_train)\n","\n","data = open(\"/content/drive/MyDrive/dataset/dataset/data/test-label.dat\")\n","y_test = np.loadtxt(data, delimiter=\" \", dtype='int')\n","#print(y_test)\n","\n","vocab = pd.read_csv('/content/drive/MyDrive/dataset/dataset/data/vocabs.txt', header=None)"],"metadata":{"id":"QMva_cURGj1I","executionInfo":{"status":"ok","timestamp":1651416854461,"user_tz":-180,"elapsed":53409,"user":{"displayName":"Ερμης Αρβανιτης","userId":"15428110991140850064"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Scaling the training and testing data\n","# Performing centering\n","x_train = x_train - x_train.mean()\n","print(x_train)\n","y_train = y_train - y_train.mean()\n","print(y_train)\n","# Performing Normalization\n","# scaler = Normalizer()\n","# x_train = scaler.fit_transform(x_train)\n","# print(x_train)\n","# y_train = scaler.fit_transform(y_train)\n","# print(y_train)\n","# Performing Standardization\n","# scaler = StandardScaler()\n","# x_train = scaler.fit_transform(x_train)\n","# print(x_train)\n","# y_train = scaler.fit_transform(y_train)\n","# print(y_train)"],"metadata":{"id":"0ZR-zDqYHMPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch = 20\n","splits = 5\n","kfold = KFold(n_splits=splits, shuffle=True, random_state=2)\n","\n","loss = []\n","val_loss = []\n","accuracy = []\n","val_accuracy = []\n","\n","for i, (train, test) in enumerate(kfold.split(x_train)):\n","  model = tf.keras.models.Sequential()\n","  #model.add(tf.keras.layers.Flatten())\n","  model.add(tf.keras.layers.Flatten(input_shape=(8520,1)))\n","  #model.add(tf.keras.layers.Dense(units=20, activation=tf.nn.relu))\n","  model.add(tf.keras.layers.Dense(units=4270, activation=tf.nn.relu))\n","  model.add(Dropout(0.9))\n","  #model.add(tf.keras.layers.Dense(units=2135, activation=tf.nn.relu))\n","  #model.add(tf.keras.layers.Dense(units=4270, activation=tf.nn.relu))\n","  #model.add(tf.keras.layers.Dense(units=5124, activation=tf.nn.relu))\n","  #model.add(tf.keras.layers.Dense(units=8540, activation=tf.nn.relu))\n","  model.add(tf.keras.layers.Dense(units=20, activation=tf.nn.sigmoid))\n","  #model.compile(optimizer='SGD', loss='mean_squared_error', metrics=['accuracy'])\n","  #model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n","  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.6), loss='mean_squared_error', metrics=['accuracy'])\n","  #model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.6), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","  # x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=30, train_size=0.2)\n","\n","  # callback = [EarlyStopping(monitor='val_accuracy', patience=5)]\n","  history = model.fit(x_train, y_train, epochs=epoch, validation_data=(x_test, y_test), batch_size=32)\n","  los, acc = model.evaluate(x_test, y_test)\n","  loss.append(history.history['loss'])\n","  val_loss.append(history.history['val_loss'])\n","  accuracy.append(history.history['accuracy'])\n","  val_accuracy.append(history.history['val_accuracy'])\n","\n","  print(loss)\n","  print(val_loss)\n","  print(acc)\n","  print(val_accuracy)\n","\n","# Creating the mean values for the cross validation\n","mean_loss = []\n","mean_val_loss = []\n","mean_accuracy = []\n","mean_val_accuracy = []\n","j=0\n","for i in range(len(loss[j])):\n","    temp_loss = 0\n","    temp_val_loss = 0\n","    temp_accuracy = 0\n","    temp_val_accuracy = 0\n","    for j in range(splits):\n","        temp_loss = temp_loss + loss[j][i]\n","        temp_val_loss = temp_val_loss + val_loss[j][i]\n","        temp_accuracy = temp_accuracy + accuracy[j][i]\n","        temp_val_accuracy = temp_val_accuracy + val_accuracy[j][i]\n","    mean_loss.append(temp_loss/ splits)\n","    mean_val_loss.append(temp_val_loss / splits)\n","    mean_accuracy.append(temp_accuracy / splits)\n","    mean_val_accuracy.append(temp_val_accuracy / splits)\n","\n","print(mean_loss)\n","print(mean_val_loss)\n","print(mean_accuracy)\n","print(mean_val_accuracy)\n","# Plot training and\n","# loss = history.history['loss']\n","# val_loss = history.history['val_loss']\n","epochs = range(1, len(mean_loss) + 1)\n","plt.plot(epochs, mean_loss, 'y', label='Training loss')\n","plt.plot(epochs, mean_val_loss, 'r', label='Validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend([\"Train\", 'Validation'], loc='upper right')\n","plt.savefig('/content/drive/MyDrive/plots/loss.png')\n","plt.savefig(\"loss.png\")\n","files.download(\"loss.png\") \n","plt.show()\n","\n","\n","# Plot training and validation accuracy values\n","# acc = history.history['accuracy']\n","# val_acc = history.history['val_accuracy']\n","plt.plot(epochs, mean_accuracy, 'y', label='Training acc')\n","plt.plot(epochs, mean_val_accuracy, 'r', label='Validation acc')\n","plt.title('Training and Validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(['Train', 'Validation'], loc='lower right')\n","plt.savefig('/content/drive/MyDrive/plots/acc.png')\n","plt.savefig(\"acc.png\")\n","files.download(\"acc.png\") \n","plt.show()\n","\n","print(los)\n","print(acc)"],"metadata":{"id":"FDoTE91VHQKZ"},"execution_count":null,"outputs":[]}]}
