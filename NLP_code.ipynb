{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IyB2IKsCE5ZU"},"outputs":[],"source":["import numpy as np\n","import re\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from pandas import concat\n","from sklearn.preprocessing import Normalizer, StandardScaler\n","from sklearn.model_selection import cross_val_score\n","import tensorflow as tf\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras.models import load_model\n","from sklearn import preprocessing\n","from sklearn.preprocessing import normalize, scale\n","from sklearn.model_selection import KFold\n","from tensorflow.compat.v1.losses import Reduction\n","from google.colab import files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZ0jzXKX114x"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4enyN-FKmVU"},"outputs":[],"source":["# Reading the training data and creating the x_train\n","data_train = pd.read_csv('/content/drive/MyDrive/dataset/data/train-data.dat', header=None)\n","x = data_train.values.tolist()\n","\n","x_train = []\n","for y in range(0, len(x)):\n","    temp = [0] * 8520\n","    sp = re.split('<\\d+>', x[y][0])\n","    while('' in sp) :\n","        sp.remove('')\n","    while(' ' in sp) :\n","        sp.remove(' ')\n","    for i in range(0, len(sp)):\n","        str = sp[i].split(' ')\n","        while('' in str) :\n","            str.remove('')\n","        for k in range(0, len(str)):\n","            temp[int(str[k])] = temp[int(str[k])] + 1\n","    x_train.append(temp)\n","\n","\n","np.savetxt('/content/drive/MyDrive/dataset/data/x_train.csv', x_train, fmt='%s', delimiter=\",\")\n","\n","# Reading the testing data and creating the x_test\n","data_test = pd.read_csv('/content/drive/MyDrive/dataset/data/test-data.dat', header=None)\n","x = data_test.values.tolist()\n","\n","x_test = []\n","for y in range(0, len(x)):\n","    temp = [0] * 8520\n","    sp = re.split('<\\d+>', x[y][0])\n","    while('' in sp) :\n","        sp.remove('')\n","    while(' ' in sp) :\n","        sp.remove(' ')\n","    for i in range(0, len(sp)):\n","        str = sp[i].split(' ')\n","        while('' in str) :\n","            str.remove('')\n","        for k in range(0, len(str)):\n","            temp[int(str[k])] = temp[int(str[k])] + 1\n","    x_test.append(temp)\n","\n","np.savetxt('/content/drive/MyDrive/dataset/data/x_test.csv', x_test, fmt='%s', delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgSGyFkv3Rwm"},"outputs":[],"source":["data = open(\"/content/drive/MyDrive/dataset/data/x_train.csv\")\n","x_train = np.loadtxt(data, delimiter=\",\")\n","#print(x_train)\n","\n","data = open(\"/content/drive/MyDrive/dataset/data/x_test.csv\")\n","x_test = np.loadtxt(data, delimiter=\",\")\n","#print(x_test)\n","\n","data = open(\"/content/drive/MyDrive/dataset/data/train-label.dat\")\n","y_train = np.loadtxt(data, delimiter=\" \", dtype='int')\n","#print(y_train)\n","\n","data = open(\"/content/drive/MyDrive/dataset/data/test-label.dat\")\n","y_test = np.loadtxt(data, delimiter=\" \", dtype='int')\n","#print(y_test)\n","\n","vocab = pd.read_csv('/content/drive/MyDrive/dataset/data/vocabs.txt', header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoQwcmeVZhf2"},"outputs":[],"source":["# Scaling the training and testing data\n","# Performing centering\n","x_train = x_train - x_train.mean()\n","print(x_train)\n","y_train = y_train - y_train.mean()\n","print(y_train)\n","# Performing Normalization\n","# scaler = Normalizer()\n","# x_train = scaler.fit_transform(x_train)\n","# print(x_train)\n","# y_train = scaler.fit_transform(y_train)\n","# print(y_train)\n","# Performing Standardization\n","# scaler = StandardScaler()\n","# x_train = scaler.fit_transform(x_train)\n","# print(x_train)\n","# y_train = scaler.fit_transform(y_train)\n","# print(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ic_PnCJB55N0"},"outputs":[],"source":["epoch = 30\n","splits = 5\n","kfold = KFold(n_splits=splits, shuffle=True, random_state=2)\n","\n","loss = []\n","val_loss = []\n","accuracy = []\n","val_accuracy = []\n","\n","for i, (train, test) in enumerate(kfold.split(x_train)):\n","  model = tf.keras.models.Sequential()\n","  #model.add(tf.keras.layers.Flatten())\n","  model.add(tf.keras.layers.Flatten(input_shape=(8520,1)))\n","  #model.add(tf.keras.layers.Dense(units=20, activation=tf.nn.relu))\n","  model.add(tf.keras.layers.Dense(units=4270, activation=tf.nn.relu))\n","  #model.add(tf.keras.layers.Dense(units=2135, activation=tf.nn.relu))\n","  #model.add(tf.keras.layers.Dense(units=4270, activation=tf.nn.relu))\n","  #model.add(tf.keras.layers.Dense(units=5124, activation=tf.nn.relu))\n","  #model.add(tf.keras.layers.Dense(units=8540, activation=tf.nn.relu))\n","  model.add(tf.keras.layers.Dense(units=20, activation=tf.nn.sigmoid))\n","  #model.compile(optimizer='SGD', loss='mean_squared_error', metrics=['accuracy'])\n","  #model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n","  #model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])\n","  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.6), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","  # x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=30, train_size=0.2)\n","\n","  # callbacks = [EarlyStopping(monitor='val_accuracy', patience=3)]\n","  history = model.fit(x_train, y_train, epochs=epoch, validation_data=(x_test, y_test), batch_size=32)\n","  los, acc = model.evaluate(x_test, y_test)\n","  loss.append(history.history['loss'])\n","  val_loss.append(history.history['val_loss'])\n","  accuracy.append(history.history['accuracy'])\n","  val_accuracy.append(history.history['val_accuracy'])\n","\n","  print(loss)\n","  print(val_loss)\n","  print(acc)\n","  print(val_accuracy)\n","\n","  \n","mean_loss = []\n","mean_val_loss = []\n","mean_accuracy = []\n","mean_val_accuracy = []\n","for i in range(epoch):\n","    temp_loss = 0\n","    temp_val_loss = 0\n","    temp_accuracy = 0\n","    temp_val_accuracy = 0\n","    for j in range(splits):\n","        temp_loss = temp_loss + loss[j][i]\n","        temp_val_loss = temp_val_loss + val_loss[j][i]\n","        temp_accuracy = temp_accuracy + accuracy[j][i]\n","        temp_val_accuracy = temp_val_accuracy + val_accuracy[j][i]\n","    mean_loss.append(temp_loss/ splits)\n","    mean_val_loss.append(temp_val_loss / splits)\n","    mean_accuracy.append(temp_accuracy / splits)\n","    mean_val_accuracy.append(temp_val_accuracy / splits)\n","\n","print(mean_loss)\n","print(mean_val_loss)\n","print(mean_accuracy)\n","print(mean_val_accuracy)\n","# Plot training and\n","epochs = range(1, len(mean_loss) + 1)\n","plt.plot(epochs, mean_loss, 'y', label='Training loss')\n","plt.plot(epochs, mean_val_loss, 'r', label='Validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend([\"Train\", 'Validation'], loc='upper right')\n","plt.savefig('/content/drive/MyDrive/plots/loss.png')\n","plt.savefig(\"loss.png\")\n","files.download(\"loss.png\") \n","plt.show()\n","\n","\n","# Plot training and validation accuracy values\n","plt.plot(epochs, mean_accuracy, 'y', label='Training acc')\n","plt.plot(epochs, mean_val_accuracy, 'r', label='Validation acc')\n","plt.title('Training and Validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(['Train', 'Validation'], loc='lower right')\n","plt.savefig('/content/drive/MyDrive/plots/acc.png')\n","plt.savefig(\"acc.png\")\n","files.download(\"acc.png\") \n","plt.show()\n","\n","print(los)\n","print(acc)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of Untitled2.ipynb","provenance":[{"file_id":"1h8FdIS6kuKCRB3pATx_60MdtPJ_F96D_","timestamp":1652046705817}],"authorship_tag":"ABX9TyP7+FPVQAbUJ1SSV/0I4dvp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}